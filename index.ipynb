{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![WordCloud](./img/wordcloud.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideInput\n",
    "try:\n",
    "    username\n",
    "except NameError:\n",
    "    username = 'Lernende(r)'\n",
    "\n",
    "print('Hallo, ' + username + '!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lernmodul zur Verarbeitung und Analyse von Textdaten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Verarbeiten menschlicher Sprache gilt als Voraussetzung für eine erfolgreiche Mensch-Maschine-Kommunikation. Zusätzlich kann sie dabei helfen eine vom Mensch unternommene Textanalyse zu unterstützen. Handelt es sich bei diesem Kommentar um unerwünschte Inhalte? Verbreitet dieser Beitrag Falschinformationen? Und welche Meinung will der Verfasser mit dieser Rezension zum Ausdruck bringen? Jene Fragestellungen lassen sich mithilfe computergestützter Methoden der Textverarbeitung und -analyse beantworten.\n",
    "\n",
    "Im folgenden Lernmodul wird versucht im Zuge einer Stimmungsanalyse (Sentiment Analysis) die Intention hinter einem Textbeitrag zu identifizieren um bspw. zwischen einer negativen und einer positiven Aussage zu unterscheiden. Diese Unterscheidung übernimmt ein Modell (hier: Klassifikator), welches auf Basis von Metadaten (hier: Vokabular) eine Vorhersage (hier: Stimmung) über vorliegende Rohdaten (hier: Tweets) trifft.\n",
    "\n",
    "Die folgende Abbildung verdeutlicht dabei den Weg eines Tweets, welcher die drei Domänen [Daten](#daten), [Modell](#modell) und [Verwendung](#verwendung) durchläuft, bis er letztendlich einer Stimmung zugeordnet wurde:\n",
    "\n",
    "![Pipeline](./img/flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Übersicht über die Lerninhalte:**\n",
    "\n",
    "1. [Einführung](#1.-Einführung)\n",
    "    1. [Motivation](#1.1-Motivation)\n",
    "    2. [Voraussetzungen](#1.2-Voraussetzungen)\n",
    "2. [Daten](#2.-Daten)\n",
    "    1. [Textdaten beschaffen](#2.1-Textdaten-beschaffen)\n",
    "    2. [Textdaten erkunden](#2.2-Textdaten-erkunden)\n",
    "    3. [Textdaten aufbereiten](#2.3-Textdaten-aufbereiten)\n",
    "        1. [Zeichen bereinigen](#2.3.1-Zeichen-bereinigen)\n",
    "        2. [Wörter bereinigen](#2.3.2-Wörter-bereinigen)\n",
    "        3. [Tweets bereinigen](#2.3.3-Tweets-bereinigen)\n",
    "3. [Modell](#3.-Modell)\n",
    "    1. [Textdaten einbetten](#3.1-Textdaten-einbetten)\n",
    "        1. [Vokabular](#3.1.1-Vokabular)\n",
    "        2. [Vektorisierung](#3.1.2-Vektorisierung)\n",
    "        3. [Label](#3.1.3-Label)\n",
    "        4. [Features](#3.1.4-Features)\n",
    "    2. [Textdaten klassifizieren](#3.2-Textdaten-klassifizieren)\n",
    "        1. [Überwachte Klassifikation](#3.2.1-Überwachte-Klassifikation)\n",
    "        2. [Unüberwachte Klassifikation](#3.2.2-Unüberwachte-Klassifikation)\n",
    "4. [Verwendung](#4.-Verwendung)\n",
    "    1. [Textdaten vorhersagen](#4.1-Textdaten-vorhersagen)\n",
    "        1. [Überwachter Klassifikator](#4.1.1-Überwachter-Klassifikator)\n",
    "        2. [Unüberwachter Klassifikator](#4.1.2-Unüberwachter-Klassifikator)\n",
    "5. [Abschluss](#5.-Abschluss)\n",
    "6. [Anhang](#6.-Anhang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lernziele:**\n",
    "\n",
    "Die Teilnehmer lernen in der [Datendomäne](#2.-Daten) Textdaten in roher Form Schritt für Schritt in kontextbezogene Daten für eine Textanalyse zu transformieren. Daraufhin bewegen sich die Teilnehmer in der [Modelldomäne](#3.-Modell), in der sie die Daten in das Modell einbetten um sie von zwei unterschiedlichen Algorithmen klassifizieren zu lassen. Abschließend werden die entstandenen Klassifikatoren in der [Verwendungsdomäne](#4.-Verwendung) für eine Vorhersage der Stimmung benutzt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Voraussetzungen:**\n",
    "\n",
    "- [Python](https://www.python.org/)\n",
    "- [Lernmodul zum Datenimport und zur Datenvorbereitung mit Pandas](https://projectbase.medien.hs-duesseldorf.de/eild.nrw-module/lernmodul-pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabenüberprüfung:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses Lernmodul nutzt eine Aufgabenüberprüfung, die sich hinter der folgenden Variable verbirgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideInput\n",
    "from taskreview.learning_module import LearningModule\n",
    "lm = LearningModule('data/lernmodul_texte.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hinweis:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses Lernmodul nutzt verschiedenste Bibliotheken, Objekte und Funktionen. Diese sollten in der vorgegebenen Reihenfolge ausgeführt werden, da ansonsten kein erfolgreicher Abschluss des Lernmoduls garantiert werden kann.\n",
    "\n",
    "Solltet ihr einmal den Überblick verlieren, gibt euch die `whos` Funktion alle aktiven Variablen und ihre Typen (bspw. Bibliothek, Klasse) aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erweiternd dazu führt dieses Lernmodul im [Anhang](#6.-Anhang) ein Glossar, welches englische Begriffe in ihren deutschen Kontext setzt und sie mit einem Beispiel versieht.\n",
    "\n",
    "Zudem nutzt dieses Lernmodul eine hohe Datenmenge, welche die maximal zur Verfügung stehende Speicherkapazität überschreiten kann. Um dem vorzubeugen, können ungenutzte Variablen über die `del` Funktion verworfen und so Speicherkapazitäten freigegeben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = None \n",
    "del variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #150458; padding: 5px;\"></div>\n",
    "\n",
    "## 1. Einführung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zurück zur Übersicht](#Lernmodul-zur-Verarbeitung-und-Analyse-von-Textdaten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Verarbeitung menschlicher Sprache (Natural Language Processing, NLP) ist ein prominentes Forschungsfeld der Informatik mit ähnlichem Bekanntheitsgrad wie die Analyse visueller Daten (Computer Vision), welche bspw. das Erkennen von Bildern (Image Recognition) sowie ihre Manipulation (Image Augmentation) betrachtet. Im ersten Fall wollen wir einen bekannten bzw. alten Bildinhalt klassifizieren, während der zweite Fall einen unbekannten bzw. neuen Bildinhalt erzeugt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wechseln wir von Bild- auf Textdaten, lässt sich das Forschungsfeld des _NLP_ auf ähnliche Problemstellungen herunterbrechen, die in der folgenden Auflistung aufgeführt sind:\n",
    "\n",
    "* Texterkennung (Text Recognition)\n",
    "  * Spracherkennung (Speech Recognition)\n",
    "  * Stimmungsanalyse (Sentiment Analysis)\n",
    "* Textmanipulation (Text Augmentation)\n",
    "  * Maschinelle Übersetzung (Machine Translation)\n",
    "  * Automatisches Antworten (Question Answering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses Lernmodul beschäftigt sich mit der ersten Problemstellung, also der Texterkennung und im besonderen mit der Unterscheidung verschiedener Stimmungen, welche der Verfasser eines Textes gehabt haben könnte. Ein aktuelles Anwendungsbeispiel ist die Identifikation von Hassreden, wie sie bspw. von [Facebook](https://www.facebook.com/) im Zuge ihrer [Community Standards](https://www.facebook.com/communitystandards/hate_speech) verboten wurden und erkannt werden müssen. Auch könnte man ein Stimmungsbild der Gesellschaft über die Auswertung von Beiträgen auf Twitter betrachten, welches in Krisenzeiten als Entscheidungshilfe für geplante Gegenmaßnahmen dienen kann."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Voraussetzungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses Lernmodul nimmt sich [NumPy](https://numpy.org/) für numerische Berechnungen und [Pandas](https://pandas.pydata.org/) für die Verarbeitung der Daten zur Hilfe. Letztere Bibliothek konntet ihr bereits im [Lernmodul zum Datenimport und zur Datenvorbereitung mit Pandas](https://projectbase.medien.hs-duesseldorf.de/eild.nrw-module/lernmodul-pandas) kennen lernen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dazu werden die folgenden Skripte zum Verarbeiten, Modellieren & Visualisieren der Daten benötigt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import data_functions as df\n",
    "from utils import model_functions as mf\n",
    "from utils import vis_functions as vf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apropos Daten - lasst uns gleich in der [Datendomäne](#2.-Daten) einsteigen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #150458; padding: 5px;\"></div>\n",
    "\n",
    "## 2. Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zurück zur Übersicht](#Lernmodul-zur-Verarbeitung-und-Analyse-von-Textdaten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Kapitel beschaffen wir uns zunächst einen Datensatz, welcher aus vielen Beispielen besteht, welche die Menge an Daten bildet. Ein Beispiel umfasst dabei mindestens ein, wenn nicht sogar mehrere Merkmale. Ein Merkmal wird dabei durch einen numerischen oder symbolischen Wert verkörpert, der in unserem Fall ein Textbeitrag (Inhalt eines Tweets), aber auch ein Zeitstempel (Veröffentlichungsddatum eines Tweets) sein kann. \n",
    "\n",
    "Im weiteren Verlauf dieses Kapitels werden die beschafften Daten zunächst begutachtet und auf Basis ihrer Merkmale vorgefiltert. Daraufhin werden die übrig gebliebenen Daten im Abschnitt [Textdaten aufbereiten](#2.3-Textdaten-aufbereiten) verarbeitet, um sie in der darauffolgenden [Modelldomäne](#3.-Modell) zu nutzen. Die Aufbereitung eines Textbeitrags läuft dabei exemplarisch nach folgendem Prozess ab:\n",
    "\n",
    "![Daten](./img/data_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Textdaten beschaffen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wo findet man in der heutigen Zeit aktuelle Daten, welche eine eigene Meinung enthalten? Na auf dem Kurznachrichtendienst [Twitter](https://twitter.com) natürlich! \n",
    "\n",
    "Unser Datensatz wird dabei ungefiltert von Twitter extrahiert und monatlich unter der [CC 4.0 International](https://creativecommons.org/licenses/by/4.0/legalcode) Lizenz veröffentlicht [[1]](#1).\n",
    "\n",
    "Einen Teil dieser Veröffentlichung wollen wir zunächst als rohe JSON Daten unter der `twitter` Variable wie folgt abspeichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = df.load_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Funktionalität von Pandas nutzen zu können, wird aus jeder JSON Datei ein DataFrame, der wiederum unter der `twitter` Variable abgelegt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = map(pd.DataFrame, twitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Zusammenfügen dieser resultiert in einem einzelnen DataFrame, welcher im weiteren Verlauf als Rohdatensatz fungiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = pd.concat(twitter, copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das war's auch schon mit der Beschaffung der Daten. Als nächstes gilt es die noch unbekannten Daten zu erkunden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Textdaten erkunden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie im [Lernmodul zum Datenimport und zur Datenvorbereitung mit Pandas](https://projectbase.medien.hs-duesseldorf.de/eild.nrw-module/lernmodul-pandas) kennengelernt, lässt sich über die `head()` Funktion der Kopf des Datensatzes betrachten. Wie schaut dieser aus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leider liefert die `head()` Funktion nicht die Größe des Datensatzes zurück. Wir wollen aber schließlich wissen, mit wie vielen unterschiedlichen Tweets wir rechnen können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Wie viele Beispiele sind im `twitter` Datensatz enthalten?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideInput\n",
    "lm.show_task(221)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Wie viele Werte liegen pro Beispiel vor? Aus wie vielen Merkmalen besteht der `twitter` Datensatz?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideInput\n",
    "lm.show_task(222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als erstes kümmern wir uns um die Aktualität der Daten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Über welches Merkmal können wir Tweets identifizieren, die nicht die gewünschte Aktualität besitzen?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideInput\n",
    "lm.show_task(223)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus welchen Jahren stammen unsere Tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = vf.plot_tweets_per_year(twitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anscheinend enthält unser Datensatz veraltete Tweets. Da sich unsere Fragestellung auf aktuelle Inhalte bezieht, verwerfen wir ältere Tweets wie folgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = twitter.loc[twitter['year'] == 2020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Nächstes kümmern wir uns um die Sprache, die einem jeden Tweet hinterlegt ist. Welche Sprachen sind vertreten?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter['lang'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anscheinend enthält unser Datensatz verschiedenste Sprachen. Da sich unsere Fragestellung auf deutsche Inhalte bezieht, verwerfen wir anderssprachige Tweets wie folgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = twitter.loc[twitter['lang'] == 'de']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach einer ersten Filterung der Daten konzentrieren wir uns zum Abschluss auf die gewünschten Textbeiträge. Da unser Datensatz keine relevanten Metadaten enthält, extrahieren wir lediglich die Tweets vom DataFrame `twitter` in eine DataSeries `tweets` wie folgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = twitter['text']\n",
    "del twitter\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beim Extrahieren der Tweets werden ausnahmslos alle Beispiele berücksichtigt. Das gilt auch für fehlende Tweets, die in der Menge an Beispielen untergegangen sind. Sind unsere Daten nun bereit zur Aufbereitung?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Wie viele Werte fehlen im `tweets` Datensatz?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideInput\n",
    "lm.show_task(224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da fehlende Werte von den folgenden Schritten nicht verarbeitet werden können, entfernen wir sie wie folgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Übersicht halber fügen wir ein frei erfundenes Beispiel an, um die folgenden Verarbeitungsschritte besser nachvollziehen zu können.\n",
    "\n",
    "> ⚠️ Dieser Tweet enthält viele unnütze Zeichen 😟 die wir lieber verwerfen.\n",
    "> \n",
    "> Das wurde uns von @user unter https://example.org verraten #danke\n",
    "\n",
    "Diesen ausgedachten Tweet fügen wir wie folgt an unsere Daten an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = '⚠️ Dieser Tweet enthält viele unnütze Zeichen 😟 die wir lieber verwerfen. \\n Das wurde uns von @user unter https://example.org verraten! #danke'\n",
    "tweets = tweets.append(pd.Series(example), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das war's auch schon mit der Erkundung der Daten. Bis jetzt haben wir lediglich fehlende Beispiele verworfen, nicht aber vorhandene Beispiele verändert. Bevor das passiert, behalten wir lieber eine Kopie der Daten als `tweets_copy` auf die wir im Verlauf des Lernmoduls noch zurückgreifen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_copy = tweets.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Wozu kann eine solche Kopie gut sein?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideInput\n",
    "lm.show_task(225)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Textdaten aufbereiten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Innerhalb der Datenaufbereitung werden wir jeden Tweet anhand seiner Bestandteile (bspw. Zeichen, Wörter) und Eigenschaften (bspw. Länge) verarbeiten. Exemplarische Verarbeitungsschritte wurden bereits zu [Anfang](#2.-Daten) dieses Kapitels visualisiert. Aber schauen wir uns erst einmal das eben angefügte Beispiel an."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Mit welchem Befehl kommen wir zum letzten Tweet im `tweets` Datensatz?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideInput\n",
    "lm.show_task(231)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leider enthält unser ausgedachter Tweet allerhand Elemente, die wir gesondert behandeln müssen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Zeichen bereinigen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Welche Elemente befinden sich letzten Tweet, die wir verarbeiten müssen?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideInput\n",
    "lm.show_task(232)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fangen wir mit den Emojis an. Diese werden über die `process_emojis()` Funktion wie folgt verarbeitet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df.process_emojis(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Die `process_emojis()` Funktion kümmert sich um ausgewählte Emojis. Was passiert mit ihnen?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideInput\n",
    "lm.show_task(233)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir spezielle Zeichenketten, wie die URL _https://example.org_ verarbeiten. Zusätzlich werden über die `process_strings()` Funktion Sonderzeichen wie _@_ und _\\n_ wie folgt behandelt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df.process_strings(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Die `process_strings()` Funktion kümmert sich um spezielle Zeichenketten. Was passiert mit ihnen?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideInput\n",
    "lm.show_task(234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als letztes kümmern wir uns um verbleibende Symbole (bspw. Satzzeichen) über die `process_symbols()` Funktion wie folgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df.process_symbols(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Die `process_symbols()` Funktion kümmert sich generell um Sonderzeichen. Was passiert mit ihnen?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideInput\n",
    "lm.show_task(235)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Wörter bereinigen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach der Bereinigung von Zeichen im vorherigen Abschnitt bleiben lediglich ganze Wörter übrig. Jetzt können wir jeden Tweet in seine Bestandteile zerlegen. Dieser Schritt geht mit der Funktionsweise eines [Tokenizer](https://de.wikipedia.org/wiki/Tokenizer) einher, welcher lediglich den kompletten Tweet in einzelne Token, also Wörter, aufteilt. Diese lassen sich besser verarbeiten, als ein ganzer Satz.\n",
    "\n",
    "Die Zerteilung der Zeichenkette bringt uns eine Liste von Wörtern innerhalb eines Tweets. Dabei werden alle Wörter zusätzlich in Kleinschreibung abgebildet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.apply(str.lower).apply(str.split)\n",
    "tweets.iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt besteht jeder Tweet nicht mehr nur aus einer Zeichenkette, sondern aus mehreren Wörtern. Aus Analysegründen fassen wir alle Wörter zu einer großen Sammlung unter `all_words` zusammen, was die `aggregate_words()` Funktion wie folgt umsetzt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_words(tweets):\n",
    "    \"\"\"Aggregate words from tweets\"\"\"\n",
    "    \n",
    "    all_words = []\n",
    "    for sentence in tweets:\n",
    "        for word in sentence:\n",
    "            all_words.append(word)\n",
    "            \n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = aggregate_words(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Eine Sammlung aller Wörter findet sich in der `all_words` Liste. Wie viele unterschiedliche Wörter sind getweetet worden?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideInput\n",
    "lm.show_task(236)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "all_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem wir alle Wörter gesammelt haben, interessieren uns die beliebtesten Wörter der Tweeter. An dieser Stelle greifen wir auf das Natural Language Toolkit ([NLTK](https://www.nltk.org/)) zurück, welches eine Häufigkeitsverteilung einer Liste über die `FreqDist()` Funktion wie folgt erstellt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fd = FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Die `fd` Verteilung basiert auf allen getweeteten Wörtern. Wie lauten die drei häufigsten Wörter?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideInput\n",
    "lm.show_task(237)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es ist nicht überraschend, dass sich Artikel, Pronomen oder Konjunktionen als die häufigsten Wörter entpuppen. Schauen wir also etwas genauer hin und betrachten die 30 beliebtesten Wörter wie folgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd.plot(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch hier scheinen Artikel, Pronomen und Konjunktionen stark vertreten zu sein. Das bringt uns zu der Frage, welchen Inhalt bzw. welche Stimmung diese Wörter eigentlich kommunizieren?\n",
    "\n",
    "Jene Wörter, werden im Kontext des _NLP_ als Stoppwörter bezeichnet, da sie keine Wertung implizieren, wie es ein Adjektiv (bspw. gut, schlecht) tun würde. Stoppwörter lassen sich daher ähnlich wie das Rauschen aus einer Nachricht (hier: Tweet) entfernen, ohne dass die Information (hier: Stimmung) dieser Nachricht verloren geht.\n",
    "\n",
    "Wir werden also im folgenden Schritt alle Stoppwörter entfernen. Um sie zu identifizieren, importieren wir sie über die `load_stop_words()` Funktion in eine Liste `stop_words` wie folgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = df.load_stop_words()\n",
    "stop_words.Wort.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hätte man lediglich die meist genutzten Wörter aus der `fd` Verteilung genommen, wäre man Gefahr gelaufen, relevante Wörter (bspw. schön), ebenfalls zu entfernen.\n",
    "\n",
    "Bevor wir alle Stoppwörter aus den Tweets entfernen, fragen wir uns aber erstmal, wie viel Prozent unseres Datensatzes wir dadurch verlieren würden? Dies lässt sich über die `stop_words_ratio()` Funktion wie folgt berechnen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words_ratio(text):\n",
    "    \"\"\"Count stopwords in tweets\"\"\"\n",
    "    \n",
    "    stop_words_list = stop_words['Wort'].values.tolist()\n",
    "    content = [word for word in text if word not in stop_words_list]\n",
    "    \n",
    "    return 1 - len(content) / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_ratio(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Stoppwörter endgültig aus unseren Tweets zu entfernen, werfen wir die `filter_tokens()` Funktion wie folgt an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tokens(tweets):\n",
    "    \"\"\"Filter tokens in tweets\"\"\"\n",
    "    \n",
    "    row_list = []\n",
    "    stop_words_list = stop_words['Wort'].values.tolist()  \n",
    "    for row in tweets:\n",
    "        \n",
    "        token_list = []\n",
    "        for token in row:\n",
    "            \n",
    "            if token not in stop_words_list:\n",
    "                if len(token) > 1:\n",
    "                    token_list.append(token)\n",
    "                    \n",
    "        row_list.append(token_list)\n",
    "    \n",
    "    return pd.Series(row_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = filter_tokens(tweets)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit ist unsere Sammlung von Wörter nicht mehr aktuell. Der Einfachheit halber sammeln wir sie erneut über die `aggregate_words()` Funktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = aggregate_words(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir eine Aussage darüber treffen, welche Wörter mit Ausnahme der Stoppwörter am häufigsten verwendet werden? Dazu ließe sich wiederum die `FreqDist()` Funktion nutzen. \n",
    "\n",
    "Alternativ zur bereits bekannten Häufigkeitsverteilung lassen sich im Kontext des _NLP_ sog. Schlagwortwolken visualisieren. An dieser Stelle greifen wir auf die [WordCloud](https://amueller.github.io/word_cloud/) Bibliothek wie folgt zurück:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wc = WordCloud()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie sehen nun die häufigsten Wörter aus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_words = wc.generate_from_frequencies(FreqDist(all_words))\n",
    "del all_words\n",
    "vf.plot_image(most_common_words, 'Die häufigsten Wörter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Tweets bereinigen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da uns der vergangene Abschnitt einige Wörter gekostet hat, wie die `stop_words_ration()` Funktion verriet, wollen wir nun verbliebenen Wörter pro Tweet zählen. Ein Tweet, der nur noch wenige Wörter lang ist, verkörpert keine qualitativen Daten mehr. Da Twitter kein Zeichenminimum pro Tweet setzt, kann unser Datensatz auch Tweets enthalten, die wenig bis gar keinen Inhalt repräsentieren. Um solche Tweets zu identifizieren berechnen wir die Wortlänge aller Tweets wie folgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_length = [len(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Die `tweets_length` Liste enthält die Anzahl aller Wörter eines Tweet. Wie viele Wörter besitzt der kürzeste Tweet im Datensatz?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideInput\n",
    "lm.show_task(238)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "tweets_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoffentlich handelt es sich um einen Ausreißer. Das verifizieren wir wiederum mit einer Häufigkeitsverteilung. Dieses Mal aber als einfaches Histogramm über die `plot_tweets_per_length()` Funktion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf.plot_tweets_per_length(tweets_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die meisten Tweets scheinen nach Entfernen der Stoppwörter nur noch wenige Wörter lang zu sein. Wir behalten lediglich Tweets, die zwei oder mehr Wörter beinhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets[tweets.map(len) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zeit für ein Status Quo: wie groß sind unsere Verluste nach Bereinigen von Zeichen, Wörtern und zu kurz geratenen Tweets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Wie viel Prozent bleiben nach Aufbereitung der Daten im Vergleich zu den ursprünglichen Tweets übrig?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideInput\n",
    "lm.show_task(239)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit dem bereinigten Datensatz geht es nun weiter in die [Modelldomäne](#3.-Modell), in der wir uns zu aller erst um die Überführung der Daten in das Modell kümmern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #150458; padding: 5px;\"></div>\n",
    "\n",
    "## 3. Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zurück zur Übersicht](#Lernmodul-zur-Verarbeitung-und-Analyse-von-Textdaten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Kapitel beschäftigen wir uns zunächst mit der Klassifikation der bereinigten Daten. NLTK ermöglicht uns die Verwendung der folgenden Klassifikationsarten:\n",
    "\n",
    "* Überwachte Klassifikation mittels Naive Bayes\n",
    "* Unüberwachte Klassifikation mittels K-means Clustering\n",
    "\n",
    "Je nach Klassifikationsart bedarf es unterschiedlicher Schritte um die Textdaten durch ein Modell klassifizieren zu lassen. Beiden Modellen ist gemein, dass sie nicht einfach mit einer Liste von Wörtern gefüttert werden können, wie sie aus der [Datendomäne](#2.-Daten) herauskommt. Daher beschäftigt sich der erste Abschnitt dieses Kapitels mit der Überführung der Daten in ein Modell. Die folgende Abbildung verdeutlicht obligatorische Schritte, die für eine Überführung benötigt werden: \n",
    "\n",
    "![Model](./img/model_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Textdaten einbetten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da unsere bereinigten Daten weiterhin als Text vorliegen, benötigen wir als erstes eine Überführung der Daten in eine numerische Form, die von unserem Modell (hier: Klassifikator) verstanden wird. Man spricht in diesem Zusammenhang vom Einbetten ([Embedding](https://en.wikipedia.org/wiki/Word_embedding)) der Wörter.\n",
    "\n",
    "Es gibt unterschiedliche Verfahren, um das symbolische Wort auf einen numerischen Wert abzubilden, deren Behandlung für dieses Lernmodul aber zu weit gingen. Daher schlagen wir die Wörter einfach in einer Art Vokabular nach, das für jedes Wort einen eigenen Wert besitzt. Wie sieht solch ein Vokabular aus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Vokabular "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Vokabular wurde über die Universität Leipzig unter der [CC BY-NC-SA 3.0 DE](https://creativecommons.org/licenses/by-nc-sa/3.0/de/) Lizenz veröffentlicht [[2]](#2) und besteht aus negativ bzw. positiv annotierten Wörtern der deutschen Sprache. Die Stimmungslage ist im Intervall von `[-1, 1]` bzw. `['negativ', 'positiv']` begrenzt. Schlagen wir negative Wörter, wie _Gefahr_ bzw. _Schuld_ im Vokabular nach, bekommen wir einen negativen Stimmungswert zurückgegeben. Je positiver das Wort, desto höher ist sein Stimmungswert. Neutrale Wörter besitzen jedoch keine Gewichtung, ihr Stimmungswert ist gleich Null.\n",
    "\n",
    "Unser Modell interessiert sich dabei lediglich für die zwei Extrema und soll als binärer Klassifikator zwischen negativen und posiven Tweets differenzieren. Wir vergeben daher die Labels für unsere zwei Klassen wie folgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['negativ', 'positiv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst holen wir uns die negativen Wörter für das Vokabular über die `load_vocabulary()` Funktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_vocabulary = df.load_vocabulary('negativ')\n",
    "negative_vocabulary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derselbe Weg führt uns zum positiven Vokabular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_vocabulary = df.load_vocabulary('positiv')\n",
    "positive_vocabulary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ähnlich wie beim Einlesen des `twitter` Datensatzes, führen wir das negative und das positive Vokabular im DataFrame `vocabulary` zusammen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = pd.concat([negative_vocabulary, positive_vocabulary])\n",
    "del negative_vocabulary, positive_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie bereits für die Tweets halten wir nach fehlenden Werten Ausschau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: In welcher Spalte des Vokabulars fehlen Werte?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideInput\n",
    "lm.show_task(311)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anstatt sie zu streichen, werden fehlende Werte mit einer leeren Zeichenkette ersetzt. Dies verhindert, dass unser Vokabular schrumpft, was wir unbedingt verhindern wollen, da unser Modell auf ein vollständiges Vokabular angewiesen ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun kümmern wir uns um ein paar Verschönerungen mithilfe der `format_vocabulary()` Funktion. Dabei werden insbesondere alle Deklinationsformen in eine eigene Reihe überführt und mit ihrem zugehörigen Stammwort versehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = df.format_vocabulary(vocabulary)\n",
    "vocabulary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das fertige Vokabular wollen wir nun genauer betrachten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Welche Wort-Typen sind im Vokabular vorhanden?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideInput\n",
    "lm.show_task(312)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Wort im Vokabular verkörpert entweder eine Stamm-, oder eine Deklinationsform. Letztere dienen der Verallgemeinerung des Vokabulars, da Stammformen im Text seltener vorkommen, als ihre zugehörigen Deklinationsformen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Wie viele Deklinationsformen sind im Vokabular vorhanden?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideInput\n",
    "lm.show_task(313)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Fällt das Vokabular eher negativ oder eher positiv aus?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideInput\n",
    "lm.show_task(314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Welchen Stimmungswert besitzt das Adjektiv _gut_ im Vokabular?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideInput\n",
    "lm.show_task(315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Wie lautet das \"schlechteste\", also das am schlechtesten annotierte Wort im  Vokabular?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideInput\n",
    "lm.show_task(316)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Abschluss visualisieren wir exemplarisch alle negativen Nomen mithilfe der bereits bekannten Schlagwortwolke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_negative_nouns = wc.generate_from_frequencies(df.filter_vocabulary(vocabulary, sentiment='negativ', word='NN'))\n",
    "vf.plot_image(most_negative_nouns, 'Die negativsten Nomen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bevor wir uns wieder unseren Tweets widmen, schauen wir noch auf die positiven Adjektive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_positive_adjectives = wc.generate_from_frequencies(df.filter_vocabulary(vocabulary, sentiment='positiv', word='ADJX'))\n",
    "vf.plot_image(most_positive_adjectives, 'Die positivsten Adjektive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Vektorisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Einbetten von Wörtern geschieht innerhalb des _NLP_ über Häufigkeitsverteilungen. Je öfter ein Wort im Text vorkommt, desto wichtiger scheint es zu sein, was sich in der Vektorisierung des Wortes widerspiegelt. In unserem Fall benutzen wir anstelle der Häufigkeitsverteilung das angefertigte Vokabular in der Annahme, dass je höher ein Stimmungswert ausfällt, desto wichtiger das zugehörige Wort zu sein scheint.\n",
    "\n",
    "Daher überführen wir jedes einzelne Wort eines Tweets in seinen Stimmungswert. Das Wort _gut_ kriegt dabei den Stimmungswert _0.37_ zugewiesen, wie bereits im vorherigen Abschnitt gezeigt wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary.loc[vocabulary['Wort'] == 'gut'].Wert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für eine performante Vektorisierung benötigen wir einen Look-up-table (LUT), welcher aus Schlüsselwertpaaren der Form `( Wort: Stimmungswert )` besteht und wie folgt erzeugt wird:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lut = dict((k.lower(), v) for k, v in vocabulary[['Wort', 'Wert']].values)\n",
    "list(lut.items())[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Vektorisierung wird nun über die `vectorize()` Funktion ermöglicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tweet):\n",
    "    \"\"\"Vectorize tweets based on look-up-table\"\"\"\n",
    "    \n",
    "    return [lut.get(word) for word in tweet if lut.get(word)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da wir die ursprünglichen Textdaten behalten wollen, brauchen wir eine zusätzliche Spalte und damit einen DataFrame statt einer DataSeries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.to_frame(name='Token')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Abschluss vektorisieren wir mithilfe der `vectorize()` Funktion unsere Textdaten wie folgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['Vector'] = tweets['Token'].apply(vectorize)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Während unsere Tweets lediglich als einzelne Wörter bzw. Vektoren vorliegen, benötigt ein überwachter Klassifikator zusätzlich eine Klasse (hier: Label), die einem Tweet zugeordnet ist. Wie bereits bekannt wollen wir eine binäre Klassifikation durchführen, wonach es zwei unterschiedliche Klassen zuzuordnen gilt. \n",
    "\n",
    "Erinnern wir uns zurück an die Stimmungslage im Intervall `['negativ', 'positiv']` annotiert die `label()` Funktion im folgenden jeden Tweet auf Basis seiner summierten Stimmungswerte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(tweet):\n",
    "    \"\"\"Label tweets according to sentiment score.\"\"\"\n",
    "    \n",
    "    score = sum(tweet)\n",
    "            \n",
    "    if score > 0.0:\n",
    "        return 'positiv'\n",
    "    elif score < 0.0:\n",
    "        return 'negativ'\n",
    "    else:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['Label'] = tweets['Vector'].apply(label)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Wann bekommt ein Tweet durch die `label()` Funktion eine neutrale Stimmung zugewiesen?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideInput\n",
    "lm.show_task(317)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neutrale Tweets sind für unsere binäre Klassifikation nicht relevant, da sie eine dritte Klasse verkörpern. Sie werden somit verworfen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets[tweets['Label'] != 'neutral']\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun ist jedem Tweet entweder eine negative oder eine positive Stimmung zugewiesen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Wie sieht die Verteilung zwischen negativen und positiven Tweets aus? Fällt die Stimmungslage eher negativ oder eher positiv aus?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideInput\n",
    "lm.show_task(318)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "negative_vocabulary_count = 0\n",
    "positive_vocabulary_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Abschluss visualisieren wir das gerade erzeugte Stimmungsbild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf.plot_pie([negative_vocabulary_count, positive_vocabulary_count], labels=labels, title='Stimmungsbild')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4 Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unser überwachter Klassifikator ist bekanntermaßen auf Features und Labels angewiesen. Während wir zweitere im vorherigen Abschnitt erzeugt haben, kümmern wir uns nun um die Features eines Tweets. In diesem Fall besteht ein einzelnes Feature wiederum aus einem Schlüsselwertpaar der Form `{ Wort: Präsenz }`, das angibt, ob ein Wort in einem Tweet vorhanden ist. Da unser überwachter Klassifikator alle Features gleich behandelt, ist dies für einen Tweet generell gegeben. Damit repräsentiert ein Tweet in unserem Fall genau so viele Features, wie er Wörter besitzt. \n",
    "\n",
    "Solche Single-Word-Features werden also als Schlüsselwertpaare über die `featurize()` Funktion wie folgt zusammengestellt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(tweet):\n",
    "    \"\"\"Single word feature as key/value pair\"\"\"\n",
    "    \n",
    "    return dict([(word, True) for word in tweet])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Zusammenfassen mehrere Features übernimmt die `feature_set()` Funktion. An dieser Stelle werden alle Single-Word-Features eines Tweets gesammelt und das Label (hier: Stimmung) für diesen Tweet als Tupel der Form `( Features, Label )` angehangen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_set(tweets, label):\n",
    "    \"\"\"Feature set as feature/label tuples\"\"\"\n",
    "    \n",
    "    features = tweets.loc[tweets['Label'] == label]['Token'].apply(featurize)\n",
    "    \n",
    "    return [(tweet_dict, label) for tweet_dict in features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Menge aller Feature-Sets repräsentiert somit die Menge aller Tweets und damit unsere Trainingsdaten für den überwachten Klassifikator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = feature_set(tweets, 'negativ') + feature_set(tweets, 'positiv')\n",
    "train_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Textdaten klassifizieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie bereits behandelt wollen wir unsere Textdaten anhand zweier unterschiedlicher Modelle klassifizieren. Die folgende Abbildung verdeutlicht den Unterschied zwischen überwachten und unüberwachten Lernen:\n",
    "\n",
    "![Image](https://datasolut.com/wp-content/uploads/2020/08/Supervised-vs-unsupervised-learning.png)\n",
    "\n",
    "Während das überwachte Lernen auf unterschiedlichen Klassen (hier: Kreis und Kreuz) basiert, benötigt unüberwachtes Lernen keine explizite Einteilung in Klassen mithilfe von Labels. Der Vorteil liegt auf der Hand: von den vielen Schritten im vergangenen Abschnitt wäre lediglich die Vektorisierung für eine unüberwachte Klassifikation notwendig gewesen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Überwachte Klassifikation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mithilfe des [Naive Bayes](https://www.nltk.org/_modules/nltk/classify/naivebayes.html) Algorithmus führen wir als erstes eine überwachte Klassifikation aus. Dabei versucht der Klassifikator eine Menge an Features ihrer wahrscheinlichsten Klasse (hier: Stimmung) zuzuordnen. Wie im vorherigen Abschnitt behandelt wurde, repräsentiert jedes Wort eines Tweets ein Feature, während der Menge aller Features (Feature-Set) die zugehörige Klasse eines Tweets angehangen ist. In der folgenden Animation wird der Lernprozess des Klassifikators auf Basis von drei Klassen und zwei Features veranschaulicht:\n",
    "\n",
    "![Naive Bayes](https://upload.wikimedia.org/wikipedia/commons/b/b4/Naive_Bayes_Classifier.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden wird der `nbclassifier` als überwachter Klassifikator mithilfe von NLTK erzeugt und trainiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import NaiveBayesClassifier, classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbclassifier = NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun stellt sich die Frage, welches Feature wohl am informativsten für den Klassifikator war?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Welches Wort aus den Trainingsdaten war für den überwachten Klassifikator am aussagekräftigsten?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideInput\n",
    "lm.show_task(321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "nbclassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt wollen wir noch wissen, wie gut sich unser Klassifikator schlägt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Wie hoch ist die Genauigkeit des überwachten Klassifikators auf den Trainingsdaten?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideInput\n",
    "lm.show_task(322)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Unüberwachte Klassifikation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mithilfe von [K-means](https://www.nltk.org/_modules/nltk/cluster/kmeans.html) Clustering führen wir als zweites eine unüberwachte Klassifikation aus. Dabei versucht der Klassifikator ähnliche Vektoren zu einem Cluster (hier: Stimmung) zu gruppieren. Wie bereits behandelt, wurde jedes Wort eines Tweets auf einen Stimmungswert abgebildet, wonach alle _n_ Wörter eines Tweets einen _n_-dimensionalen Spaltenvektor bilden. In der folgenden Animation wird der Lernprozess des Klassifikators auf Basis von drei Clustern und zwei Dimensionen veranschaulicht:\n",
    "\n",
    "![K-means](https://camo.githubusercontent.com/77a842161f9588166625169d1f0944e838837f19b105d7e55d235652cfcc3786/68747470733a2f2f692e696d6775722e636f6d2f6b3458636170492e676966)\n",
    "\n",
    "Natürlich sind nicht immer gleich viele Wörter pro Tweet vorhanden. Daher müssen wir uns auf eine Dimensionalität einigen, die vom Klassifikator berücksichtigt wird. Naheliegend entscheiden wir uns für die im Mittel gefundenen Wörter und verdoppeln diese Anzahl als `dimensionality` wie folgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensionality = 2 * int(np.mean([len(vector) for vector in tweets['Vector']]))\n",
    "dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun füllen wir entweder fehlende Werte auf, oder schneiden zu viele Werte ab, was über die `pad()` Funktion erreicht wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(tweet):\n",
    "    \"\"\"Pad vectors with zeros at the end or strip vectors based on dimensionality\"\"\"\n",
    "    \n",
    "    tweet += [0.0] * (dimensionality - len(tweet))\n",
    "    \n",
    "    return tweet[:dimensionality]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['Vector'] = tweets['Vector'].apply(pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden wird der `kmclusterer` als unüberwachter Klassifikator mithilfe von NLTK erzeugt..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.cluster import KMeansClusterer, euclidean_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmclusterer = KMeansClusterer(2, euclidean_distance, repeats=4, avoid_empty_clusters=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...und trainiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = kmclusterer.cluster(tweets['Vector'].apply(np.array), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Während beim überwachten Klassifikator die Features untersucht werden konnten, lassen sich hier lediglich die erzeugten Cluster des Klassifikators betrachten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Hinter welcher Funktion verstecken sich die Cluster unseres `kmclusterer` Klassifikators?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hideInput\n",
    "lm.show_task(323)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "centroids = [0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Abschluss schauen wir wo die Cluster liegen und ob sich ihre Lage mit dem Wertebereich der jeweiligen Stimmungswerte deckt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf.plot_clusters(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #150458; padding: 5px;\"></div>\n",
    "\n",
    "## 4. Verwendung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zurück zur Übersicht](#Lernmodul-zur-Verarbeitung-und-Analyse-von-Textdaten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Kapitel werden zum Abschluss eigene Tweets vorhergesagt, sowie bestehende Tweets für eine exemplarische Untersuchung der Vorhersagegenauigkeit verwendet.\n",
    "\n",
    "Je nach Klassifikator sind dafür verschiedene Schritte nötig, die bereits aus den vorherigen Kapiteln bekannt sind und die in der folgenden Abbildung verdeutlicht werden:\n",
    "\n",
    "![Usage](./img/usage_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Textdaten vorhersagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplarisch wird ein frei erfundener negativer Tweet betrachtet:\n",
    "\n",
    "> Was für EiN schlechtes Lernmodul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_tweet = \"Was für EiN schlechtes Lernmodul\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ebenfalls wird ein positives Beispiel betrachtet:\n",
    "\n",
    "> Was für EiN gutes Lernmodul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweet = \"Was für EiN gutes Lernmodul\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional kann ein eigener Tweet hier eingegeben werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tweet = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im produktiven Einsatz müssten all diese Beispiele in der Datendomäne aufbereitet werden, was wir uns in diesem Fall aber ersparen und lieber auf bereits aufbereitete Tweets zurückgreifen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.merge(tweets_copy.to_frame(name='Text'), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun wählen wir 100 zufällige Exemplare wie folgt aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweets = tweets.sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einzelne zufällige Tweets finden sich folgendermaßen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['Text'].sample(1).values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Überwachter Klassifikator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um den überwachten Klassifikator für eine Vorhersage zu nutzen, wird die folgende Funktion zur Hilfe genommen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_supervised(tweet, classifier):\n",
    "    print('Tweet:', tweet, '\\nStimmung:', classifier.classify(featurize(str.split(str.lower(tweet)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welche Stimmung drückt unser negatives Beispiel nach Vorhersage der überwachten Klassifikation aus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_supervised(negative_tweet, nbclassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welche Stimmung drückt unser positives Beispiel nach Vorhersage der überwachten Klassifikation aus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_supervised(positive_tweet, nbclassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welche Stimmung drückt unser selbst gewähltes Beispiel nach Vorhersage der überwachten Klassifikation aus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_supervised(user_tweet, nbclassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie schlägt sich unser überwachter Klassifikator im Detail? Dafür klassifizieren wir beispielhaft 100 Tweets über die `classify_supervised_multiple()` Funktion. Falsche Vorhersagen lassen sich über die Spalte _Prognose_ identifizieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.classify_supervised_multiple(sample_tweets, nbclassifier, labels) # On Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Unüberwachter Klassifikator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um den unüberwachten Klassifikator für eine Vorhersage zu nutzen, wird die folgende Funktion zur Hilfe genommen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_unsupervised(tweet, classifier):\n",
    "    print('Tweet:', tweet, '\\nStimmung:', labels[kmclusterer.classify(pad(vectorize(str.split(str.lower(tweet)))))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welche Stimmung drückt unser negatives Beispiel nach Vorhersage der unüberwachten Klassifikation aus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_unsupervised(negative_tweet, kmclusterer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welche Stimmung drückt unser positives Beispiel nach Vorhersage der unüberwachten Klassifizierung aus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_unsupervised(positive_tweet, kmclusterer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welche Stimmung drückt unser selbst gewähltes Beispiel nach Vorhersage der unüberwachten Klassifikation aus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_unsupervised(user_tweet, kmclusterer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie schlägt sich unser unüberwachter Klassifikator im Detail? Dafür klassifizieren 100 zufällig ausgewählte Tweets über die `classify_unsupervised_multiple()` Funktion. Falsche Vorhersagen lassen sich über die Spalte _Prognose_ identifizieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.classify_unsupervised_multiple(sample_tweets, kmclusterer, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #150458; padding: 5px;\"></div>\n",
    "\n",
    "## 5. Abschluss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zurück zur Übersicht](#Lernmodul-zur-Verarbeitung-und-Analyse-von-Textdaten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Abschluss des Lernmoduls folgen einige Verständnisfragen, bevor deine Gesamtpunktzahl errechnet wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Welche Eingabe(n) benötigt der überwachte Klassifikator?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.show_task(501)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Welche Eingabe(n) benötigt der unüberwachte Klassifikator?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.show_task(502)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Das Vokabular wird in diesem Lernmodul dafür verwendet...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.show_task(503)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Welche Faktoren haben einen direkten Einfluss auf die Trainingsdauer des unüberwachten Klassifikators?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.show_task(504)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du hast es geschafft. Glückwunsch!\n",
    "\n",
    "![Applause](https://media.giphy.com/media/FnGdcQzqypBaE/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ergebnis:**\n",
    "\n",
    "Du hast folgende Punktzahl erreicht..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.get_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submitButton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zusammenfassung:**\n",
    "\n",
    "Das vergangene Lernmodul gab einen Einblick in die Texterkennung als Teilgebiet des _NLP_. Im Zuge einer Stimmungsanalyse von aktuellen Beiträgen auf Twitter, haben wir in der [Datendomäne](#2.-Daten) zunächst Rohdaten beschafft und erkundet. Irrelevante Inhalte wurden auf Basis von Sonderzeichen, Stoppwörtern und der Länge des resultierenden Beitrags, identifiziert und verworfen. Daraufhin wurden die Daten in der [Modelldomäne](#3.-Modell) entweder überwacht oder unüberwacht klassifiziert, wobei verschiedene Einbettungsschritte, wie das Vektorisieren oder das Annotieren der Textdaten vorausgingen. Letztendlich konnte in der [Verwendungsdomäne](#4.-Verwendung) sowohl der überwachte als auch der unüberwachte Klassifikator die Stimmung eines Tweets korerkt vorhersagen. Jene Vorhersage ließe sich mithilfe alternativer Modelle, wie sie bspw. im [Lernmodul zu Datamining mit Scikit Learn](https://projectbase.medien.hs-duesseldorf.de/eild.nrw-module/lernmodul-scikit) behandelt werden, verbessern. Analog dazu können Schritte, wie der Tokenizer, oder das Embedding, unabhängig vom Modell optimiert werden. Weiterführendes Material dazu befindet sich im [Anhang](#6.-Anhang)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #150458; padding: 5px;\"></div>\n",
    "\n",
    "## 6. Anhang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zurück zur Übersicht](#Lernmodul-zur-Verarbeitung-und-Analyse-von-Textdaten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.giphy.com/media/l2Je66zG6mAAZxgqI/giphy.gif\" alt=\"Vokabular\" style=\"width: 256px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Glossar:**\n",
    "\n",
    "* Embedding\n",
    "    * Einbetten von Wörtern in Modell (hier: Klassifikator)\n",
    "* Label\n",
    "    * Klasse (hier: Stimmung)\n",
    "* Look-up-table\n",
    "    * Schlüsselwertpaar (hier: Vokabular)\n",
    "* NLP (Natural Language Processing)\n",
    "    * Verarbeitung menschlicher Sprache\n",
    "* NLTK (Natural Language ToolKit)\n",
    "    * Python-Bibliothek für die Verarbeitung menschlicher Sprache\n",
    "* Token\n",
    "    * Ein oder mehrere Wörter (hier: Monogramme)\n",
    "* Tokenization\n",
    "    * Zerteilung von Sätzen in ein oder mehrere Wörter (hier: Monogramme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weiterführende Lernmodule:**\n",
    "\n",
    "- [Lernmodul zu Datamining mit Scikit Learn](https://projectbase.medien.hs-duesseldorf.de/eild.nrw-module/lernmodul-scikit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weiterführende Inhalte:**\n",
    "\n",
    "* Leitfäden\n",
    "    * [Cookbook by NLTK](https://www.nltk.org/book/)\n",
    "    * [Recurrent neural networks by TensorFlow](https://www.tensorflow.org/guide/keras/rnn?hl=en)\n",
    "    * [Text classification by Google Developers](https://developers.google.com/machine-learning/guides/text-classification)\n",
    "* Embedding\n",
    "    * [TextBlob](https://textblob.readthedocs.io/en/dev/)\n",
    "    * [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "* Tutorials\n",
    "    * [Basic text classification by TensorFlow](https://www.tensorflow.org/tutorials/keras/text_classification?hl=en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Referenzen:**\n",
    "\n",
    "<a id=\"1\">[1]</a> Nane Kratzke. (2020). Monthly Samples of German Tweets (Version 2020-04) [Data set]. Zenodo. http://doi.org/10.5281/zenodo.3783478\n",
    "\n",
    "<a id=\"2\">[2]</a> R. Remus, U. Quasthoff & G. Heyer: SentiWS - a Publicly Available German-language Resource for Sentiment Analysis. In: Proceedings of the 7th International Language Resources and Evaluation (LREC'10), pp. 1168-1171, 2010"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
